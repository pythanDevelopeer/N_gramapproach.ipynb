{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e9ec2a9-0ee4-4e10-a74e-76d6ed4b83a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\babar\\anaconda3\\envs\\tensorfow\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\babar\\anaconda3\\envs\\tensorfow\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\babar\\anaconda3\\envs\\tensorfow\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\babar\\anaconda3\\envs\\tensorfow\\lib\\site-packages (from nltk) (2024.7.24)\n",
      "Requirement already satisfied: tqdm in c:\\users\\babar\\anaconda3\\envs\\tensorfow\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\babar\\anaconda3\\envs\\tensorfow\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "347e178c-e932-4be9-9d2a-078cec2a0f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f00a90a-5a15-45e0-8cda-7af5a5c01abf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_a</th>\n",
       "      <th>text</th>\n",
       "      <th>favorited</th>\n",
       "      <th>favoritecount</th>\n",
       "      <th>replytosn</th>\n",
       "      <th>created</th>\n",
       "      <th>truncated</th>\n",
       "      <th>replytosid</th>\n",
       "      <th>id</th>\n",
       "      <th>replytouid</th>\n",
       "      <th>statussource</th>\n",
       "      <th>screenname</th>\n",
       "      <th>retweetcount</th>\n",
       "      <th>isretweet</th>\n",
       "      <th>retweeted</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>American Harem.. #MeToo https://t.co/HjExLJdGuF</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-11-29T23:59:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.360000e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://instagram.com\" rel=\"nofollow\"&gt;...</td>\n",
       "      <td>ahmediaTV</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>@johnconyersjr  @alfranken  why have you guys ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>johnconyersjr</td>\n",
       "      <td>2017-11-29T23:59:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.360000e+17</td>\n",
       "      <td>266149840</td>\n",
       "      <td>&lt;a href=\"http://twitter.com\" rel=\"nofollow\"&gt;Tw...</td>\n",
       "      <td>JesusPrepper74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Watched Megan Kelly ask Joe Keery this A.M. if...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-11-29T23:59:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.360000e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>DemerisePotvin</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Women have been talking about this crap the en...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-11-29T23:59:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.360000e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com\" rel=\"nofollow\"&gt;Tw...</td>\n",
       "      <td>TheDawnStott</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>.@BetteMidler please speak to this sexual assa...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-11-29T23:59:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.360000e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/#!/download/ipad\" ...</td>\n",
       "      <td>scottygirl2014</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  column_a                                               text  favorited  \\\n",
       "0        1    American Harem.. #MeToo https://t.co/HjExLJdGuF        0.0   \n",
       "1        2  @johnconyersjr  @alfranken  why have you guys ...        0.0   \n",
       "2        3  Watched Megan Kelly ask Joe Keery this A.M. if...        0.0   \n",
       "3        4  Women have been talking about this crap the en...        0.0   \n",
       "4        5  .@BetteMidler please speak to this sexual assa...        0.0   \n",
       "\n",
       "  favoritecount      replytosn              created  truncated  replytosid  \\\n",
       "0             0            NaN  2017-11-29T23:59:00        0.0         NaN   \n",
       "1             0  johnconyersjr  2017-11-29T23:59:00        0.0         NaN   \n",
       "2             0            NaN  2017-11-29T23:59:00        1.0         NaN   \n",
       "3             0            NaN  2017-11-29T23:59:00        0.0         NaN   \n",
       "4            15            NaN  2017-11-29T23:59:00        0.0         NaN   \n",
       "\n",
       "             id replytouid                                       statussource  \\\n",
       "0  9.360000e+17        NaN  <a href=\"http://instagram.com\" rel=\"nofollow\">...   \n",
       "1  9.360000e+17  266149840  <a href=\"http://twitter.com\" rel=\"nofollow\">Tw...   \n",
       "2  9.360000e+17        NaN  <a href=\"http://twitter.com/download/android\" ...   \n",
       "3  9.360000e+17        NaN  <a href=\"http://twitter.com\" rel=\"nofollow\">Tw...   \n",
       "4  9.360000e+17        NaN  <a href=\"http://twitter.com/#!/download/ipad\" ...   \n",
       "\n",
       "       screenname  retweetcount  isretweet  retweeted  longitude  latitude  \\\n",
       "0       ahmediaTV           0.0        0.0        0.0        NaN       NaN   \n",
       "1  JesusPrepper74           0.0        0.0        0.0        NaN       NaN   \n",
       "2  DemerisePotvin           0.0        0.0        0.0        NaN       NaN   \n",
       "3    TheDawnStott           0.0        0.0        0.0        NaN       NaN   \n",
       "4  scottygirl2014          11.0        0.0        0.0        NaN       NaN   \n",
       "\n",
       "  location  \n",
       "0      NaN  \n",
       "1      NaN  \n",
       "2      NaN  \n",
       "3      NaN  \n",
       "4      NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_data=pd.read_csv('Documents/metoo_tweets_dec2017[1]/metoo_tweets_dec2017.csv')\n",
    "tweet_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8136ab86-4535-4ddd-9b54-675aa20614f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Download the stopwords dataset if you haven't already\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "338e2ea6-26b4-473f-964d-3ee72137a195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before stopword:        column_a                                               text  favorited  \\\n",
      "0             1    American Harem.. #MeToo https://t.co/HjExLJdGuF        0.0   \n",
      "1             2  @johnconyersjr  @alfranken  why have you guys ...        0.0   \n",
      "2             3  Watched Megan Kelly ask Joe Keery this A.M. if...        0.0   \n",
      "3             4  Women have been talking about this crap the en...        0.0   \n",
      "4             5  .@BetteMidler please speak to this sexual assa...        0.0   \n",
      "...         ...                                                ...        ...   \n",
      "398665   393131  RT @Suffragentleman: You can only choose one.....        0.0   \n",
      "398666   393132  #MeToo, say victims of sexual harassment in Ja...        0.0   \n",
      "398667   393133  Susan Collins tries to #MeToo her way out of h...        0.0   \n",
      "398668   393134  RT @OneMillionVjj: Punish those who choose not...        0.0   \n",
      "398669   393135  Chief Justice John Roberts orders misconduct r...        0.0   \n",
      "\n",
      "       favoritecount      replytosn              created  truncated  \\\n",
      "0                  0            NaN  2017-11-29T23:59:00        0.0   \n",
      "1                  0  johnconyersjr  2017-11-29T23:59:00        0.0   \n",
      "2                  0            NaN  2017-11-29T23:59:00        1.0   \n",
      "3                  0            NaN  2017-11-29T23:59:00        0.0   \n",
      "4                 15            NaN  2017-11-29T23:59:00        0.0   \n",
      "...              ...            ...                  ...        ...   \n",
      "398665             0            NaN  2017-12-25T00:00:00        0.0   \n",
      "398666             0            NaN  2017-12-25T00:00:00        0.0   \n",
      "398667             0            NaN  2017-12-25T00:00:00        0.0   \n",
      "398668             0            NaN  2017-12-25T00:00:00        0.0   \n",
      "398669             3            NaN  2017-12-25T00:00:00        0.0   \n",
      "\n",
      "        replytosid            id replytouid  \\\n",
      "0              NaN  9.360000e+17        NaN   \n",
      "1              NaN  9.360000e+17  266149840   \n",
      "2              NaN  9.360000e+17        NaN   \n",
      "3              NaN  9.360000e+17        NaN   \n",
      "4              NaN  9.360000e+17        NaN   \n",
      "...            ...           ...        ...   \n",
      "398665         NaN  9.450820e+17        NaN   \n",
      "398666         NaN  9.450820e+17        NaN   \n",
      "398667         NaN  9.450820e+17        NaN   \n",
      "398668         NaN  9.450820e+17        NaN   \n",
      "398669         NaN  9.450820e+17        NaN   \n",
      "\n",
      "                                             statussource      screenname  \\\n",
      "0       <a href=\"http://instagram.com\" rel=\"nofollow\">...       ahmediaTV   \n",
      "1       <a href=\"http://twitter.com\" rel=\"nofollow\">Tw...  JesusPrepper74   \n",
      "2       <a href=\"http://twitter.com/download/android\" ...  DemerisePotvin   \n",
      "3       <a href=\"http://twitter.com\" rel=\"nofollow\">Tw...    TheDawnStott   \n",
      "4       <a href=\"http://twitter.com/#!/download/ipad\" ...  scottygirl2014   \n",
      "...                                                   ...             ...   \n",
      "398665  <a href=\"http://twitter.com/download/android\" ...      boaomega22   \n",
      "398666  <a href=\"http://bufferapp.com\" rel=\"nofollow\">...  April_Magazine   \n",
      "398667  <a href=\"http://twitter.com\" rel=\"nofollow\">Tw...        Noofer55   \n",
      "398668  <a href=\"http://twitter.com\" rel=\"nofollow\">Tw...          ZBezzt   \n",
      "398669  <a href=\"http://twitter.com\" rel=\"nofollow\">Tw...        pewdrdad   \n",
      "\n",
      "        retweetcount  isretweet  retweeted  longitude  latitude location  \n",
      "0                0.0        0.0        0.0        NaN       NaN      NaN  \n",
      "1                0.0        0.0        0.0        NaN       NaN      NaN  \n",
      "2                0.0        0.0        0.0        NaN       NaN      NaN  \n",
      "3                0.0        0.0        0.0        NaN       NaN      NaN  \n",
      "4               11.0        0.0        0.0        NaN       NaN      NaN  \n",
      "...              ...        ...        ...        ...       ...      ...  \n",
      "398665         616.0        1.0        0.0        NaN       NaN      NaN  \n",
      "398666           0.0        0.0        0.0        NaN       NaN      NaN  \n",
      "398667           0.0        0.0        0.0        NaN       NaN      NaN  \n",
      "398668           5.0        1.0        0.0        NaN       NaN      NaN  \n",
      "398669           3.0        0.0        0.0        NaN       NaN      NaN  \n",
      "\n",
      "[398670 rows x 18 columns]\n",
      "After stopword: \n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    if isinstance(text, str):  # Check if the input is a string\n",
    "        text = text.lower()  # Convert text to lowercase\n",
    "        text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "        text = re.sub(r'@\\w+', '', text)  # Remove mentions\n",
    "        text = re.sub(r'#\\w+', '', text)  # Remove hashtags\n",
    "        text = re.sub(r'[' + string.punctuation + ']', '', text)  # Remove punctuation\n",
    "        text = re.sub(r'\\d+', '', text)  # Remove digits\n",
    "        text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
    "        text = text.strip()  # Remove leading and trailing spaces\n",
    "    else:\n",
    "        text = ''  \n",
    "    return text\n",
    "\n",
    "single_tweet = tweet_data\n",
    "cleaned_tweet = clean_text(single_tweet)\n",
    "print(f'Before stopword:',single_tweet)\n",
    "print(f'After stopword:',cleaned_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cada8179-f1ee-4775-9bfe-915ef2bab223",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7730d7a2-e66d-4dca-8c26-63090cf6520b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "315574f4-5256-419f-a76e-b44edc995380",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(cleaned_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "339fc61e-cd72-47cb-a1b8-41c872e5db15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigrams: []\n"
     ]
    }
   ],
   "source": [
    "unigrams=list(ngrams(tokens,1))\n",
    "print('Unigrams:', unigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2a7bcd45-afde-45b1-a79c-a48cb1884b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before stopword: American Harem.. #MeToo https://t.co/HjExLJdGuF\n",
      "After stopword: american harem\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    if isinstance(text, str):  # Check if the input is a string\n",
    "        text = text.lower()  # Convert text to lowercase\n",
    "        text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "        text = re.sub(r'@\\w+', '', text)  # Remove mentions\n",
    "        text = re.sub(r'#\\w+', '', text)  # Remove hashtags\n",
    "        text = re.sub(r'[' + string.punctuation + ']', '', text)  # Remove punctuation\n",
    "        text = re.sub(r'\\d+', '', text)  # Remove digits\n",
    "        text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
    "        text = text.strip()  # Remove leading and trailing spaces\n",
    "    else:\n",
    "        text = ''  \n",
    "    return text\n",
    "\n",
    "single_tweet = tweet_data.loc[0, 'text']\n",
    "cleaned_tweet = clean_text(single_tweet)\n",
    "print(f'Before stopword:',single_tweet)\n",
    "print(f'After stopword:',cleaned_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fa060ac4-dd46-490e-ab64-3f510364a02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(cleaned_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a0d7a049-1a9c-41d4-a465-6944be5de0bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigrams: [('american',), ('harem',)]\n"
     ]
    }
   ],
   "source": [
    "unigrams=list(ngrams(tokens,1))\n",
    "print('Unigrams:', unigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5dc2e305-6901-48f1-80b1-ac13d1f0e375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before stopword: We can't keep turning a blind eye and pretend this isn't real. #metoo https://t.co/1dLZcftbSs\n",
      "After stopword: we cant keep turning a blind eye and pretend this isnt real\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    if isinstance(text, str):  # Check if the input is a string\n",
    "        text = text.lower()  # Convert text to lowercase\n",
    "        text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "        text = re.sub(r'@\\w+', '', text)  # Remove mentions\n",
    "        text = re.sub(r'#\\w+', '', text)  # Remove hashtags\n",
    "        text = re.sub(r'[' + string.punctuation + ']', '', text)  # Remove punctuation\n",
    "        text = re.sub(r'\\d+', '', text)  # Remove digits\n",
    "        text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
    "        text = text.strip()  # Remove leading and trailing spaces\n",
    "    else:\n",
    "        text = ''  \n",
    "    return text\n",
    "\n",
    "single_tweet = tweet_data.loc[5, 'text']\n",
    "cleaned_tweet = clean_text(single_tweet)\n",
    "print(f'Before stopword:',single_tweet)\n",
    "print(f'After stopword:',cleaned_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "06e3a1b0-7128-4cd2-b8af-c27cc92c0f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(cleaned_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "71e6022c-6104-4faa-9c13-08c02038beb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigrams: [('we',), ('cant',), ('keep',), ('turning',), ('a',), ('blind',), ('eye',), ('and',), ('pretend',), ('this',), ('isnt',), ('real',)]\n"
     ]
    }
   ],
   "source": [
    "unigrams=list(ngrams(tokens,1))\n",
    "print('Unigrams:', unigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dabca312-dc26-43f1-8442-f8f0dca01b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading reuters: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading punkt: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import bigrams, trigrams\n",
    "from nltk.corpus import reuters\n",
    "from collections import defaultdict\n",
    "nltk.download('reuters')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "411fcc80-ac59-445d-ab20-54f9ab5f31f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next Word: market\n"
     ]
    }
   ],
   "source": [
    "words = nltk.word_tokenize(' '.join(reuters.words()))\n",
    "\n",
    "# Create trigrams\n",
    "tri_grams = list(trigrams(words))\n",
    "\n",
    "# Build a trigram model\n",
    "model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    "# Count frequency of co-occurrence\n",
    "for w1, w2, w3 in tri_grams:\n",
    "    model[(w1, w2)][w3] += 1\n",
    "\n",
    "# Transform the counts into probabilities\n",
    "for w1_w2 in model:\n",
    "    total_count = float(sum(model[w1_w2].values()))\n",
    "    for w3 in model[w1_w2]:\n",
    "        model[w1_w2][w3] /= total_count\n",
    "\n",
    "# Function to predict the next word\n",
    "def predict_next_word(w1, w2):\n",
    "    \"\"\"\n",
    "    Predicts the next word based on the previous two words using the trained trigram model.\n",
    "    Args:\n",
    "    w1 (str): The first word.\n",
    "    w2 (str): The second word.\n",
    "\n",
    "    Returns:\n",
    "    str: The predicted next word.\n",
    "    \"\"\"\n",
    "    next_word = model[w1, w2]\n",
    "    if next_word:\n",
    "        predicted_word = max(next_word, key=next_word.get)  # Choose the most likely next word\n",
    "        return predicted_word\n",
    "    else:\n",
    "        return \"No prediction available\"\n",
    "\n",
    "# Example usage\n",
    "print(\"Next Word:\", predict_next_word('the', 'free'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d6b4957b-b6dc-45ae-bed2-a8bd77cf63a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next Word: No prediction available\n"
     ]
    }
   ],
   "source": [
    "tri_grams = trigrams(words)\n",
    "import string\n",
    "words = [word.lower() for word in nltk.word_tokenize(' '.join(reuters.words())) if word not in string.punctuation]\n",
    "def laplace_smoothing(model, vocab_size, alpha=1):\n",
    "    for w1_w2 in model:\n",
    "        total_count = float(sum(model[w1_w2].values())) + alpha * vocab_size\n",
    "        for w3 in model[w1_w2]:\n",
    "            model[w1_w2][w3] = (model[w1_w2][w3] + alpha) / total_count\n",
    "def predict_next_word(w1, w2):\n",
    "    next_word = model.get((w1, w2), None)  # Use .get() to handle missing entries\n",
    "    if next_word:\n",
    "        predicted_word = max(next_word, key=next_word.get)\n",
    "        return predicted_word\n",
    "    else:\n",
    "        return \"No prediction available\"\n",
    "def predict_from_text(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    if len(words) < 2:\n",
    "        return \"Not enough words for prediction\"\n",
    "    return predict_next_word(words[-2], words[-1])\n",
    "\n",
    "print(\"Next Word:\", predict_from_text('would you like'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1dcf2735-b08f-4188-81bd-d6024ae31b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams: [('Women', 'have'), ('have', 'been'), ('been', 'talking'), ('talking', 'about'), ('about', 'this'), ('this', 'crap'), ('crap', 'the'), ('the', 'entire'), ('entire', 'time'), ('time', ','), (',', 'finally'), ('finally', 'someone'), ('someone', 'listened'), ('listened', '.'), ('.', '#'), ('#', 'metoo'), ('metoo', 'https'), ('https', ':'), (':', '//t.co/JlK11yhFXc')]\n",
      "Trigrams: [('Women', 'have', 'been'), ('have', 'been', 'talking'), ('been', 'talking', 'about'), ('talking', 'about', 'this'), ('about', 'this', 'crap'), ('this', 'crap', 'the'), ('crap', 'the', 'entire'), ('the', 'entire', 'time'), ('entire', 'time', ','), ('time', ',', 'finally'), (',', 'finally', 'someone'), ('finally', 'someone', 'listened'), ('someone', 'listened', '.'), ('listened', '.', '#'), ('.', '#', 'metoo'), ('#', 'metoo', 'https'), ('metoo', 'https', ':'), ('https', ':', '//t.co/JlK11yhFXc')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Example sentence\n",
    "sentence = tweet_data.loc[3,'text']\n",
    "\n",
    "# Tokenize the sentence\n",
    "tokens = word_tokenize(sentence)\n",
    "\n",
    "# Generate bigrams\n",
    "bigrams = list(ngrams(tokens, 2))\n",
    "\n",
    "# Generate trigrams\n",
    "trigrams = list(ngrams(tokens, 3))\n",
    "\n",
    "# Print the results\n",
    "print(\"Bigrams:\", bigrams)\n",
    "print(\"Trigrams:\", trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb486cc0-6a8f-4e93-8462-3a0e444bfaa2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
